
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder


# ### =>Importing test and train csv data

# In[2]:


data= pd.read_csv("../input/malware-detection-dk/train.csv")


# In[3]:


test= pd.read_csv("../input/malware-detection-dk/test.csv")


# In[4]:


data.head()


# ### =>let's take an idea about catogerical and non-categorical values and Null values

# In[5]:


data.info()


# ### from above, following column has more null value and less data so we will analyze it further.
# 
# - DefaultBrowsersIdentifier
# - OrganizationIdentifier
# - PuaMode 
# - Census_ProcessorClass 
# - Census_InternalBatteryType 
# - Census_IsFlightingInternal 
# - Census_ThresholdOptIn 
# -  Census_IsWIMBootEnabled
# - Unnamed: 83

# ## => Analysis of column type and missing values

# In[6]:


def update_feature_lists():
    binary = [c for c in data.columns if data[c].nunique() == 2]
    numerical = ['Census_ProcessorCoreCount',
                 'Census_PrimaryDiskTotalCapacity',
                 'Census_SystemVolumeTotalCapacity',
                 'Census_TotalPhysicalRAM',
                 'Census_InternalPrimaryDiagonalDisplaySizeInInches',
                 'Census_InternalPrimaryDisplayResolutionHorizontal',
                 'Census_InternalPrimaryDisplayResolutionVertical',
                 'Census_InternalBatteryNumberOfCharges']
    categorical = [c for c in data.columns if (c not in numerical) & (c not in binary)]
    return binary, numerical, categorical
    
binary_columns, true_numerical_columns, categorical_columns = update_feature_lists()
print(len(binary_columns))


# In[7]:


total = data.shape[0]
missing_df = []
cardinality_df = []
for col in data.columns:
    missing_df.append([col, data[col].count(), total])
    cardinality = data[col].nunique()
    if cardinality > 2 and col != 'MachineIdentifier':
        cardinality_df.append([col, cardinality])
    
missing_df = pd.DataFrame(missing_df, columns=['Column', 'Number of records', 'Total']).sort_values("Number of records", ascending=False)
cardinality_df = pd.DataFrame(cardinality_df, columns=['Column', 'Cardinality']).sort_values("Cardinality", ascending=False)
type_df = [['Binary columns', len(binary_columns)], ['Numerical columns', len(true_numerical_columns)], ['Categorical columns', len(categorical_columns)]]
type_df = pd.DataFrame(type_df, columns=['Type', 'Column count']).sort_values('Column count', ascending=True)


# In[8]:


f, ax = plt.subplots(figsize=(7, 7))
sns.barplot(x="Column count", y="Type", data=type_df, label="Missing", palette='Spectral')
plt.show()


# In[9]:


f, ax = plt.subplots(figsize=(10, 15))
sns.set_color_codes("pastel")
sns.barplot(x="Total", y="Column", data=missing_df, label="Missing", color="orange")
sns.barplot(x="Number of records", y="Column", data=missing_df, label="Existing", color="b")
ax.legend(ncol=2, loc="upper right", frameon=True)
plt.show()


# **=>method to get index of categorical and non-categorical columns**

# In[10]:


def getdataTypes(data):
    cat_index = data.dtypes[data.dtypes =='object'].index
    int_index = data.dtypes[data.dtypes !='object'].index
    return cat_index,int_index


# In[11]:


#analyse the data by groups like objects etc

#get all the int data
data_obj , int_obj = getdataTypes(data)
#print all the categorical data
data[data_obj].head()


# In[12]:



row_count, column_count = data.shape

print("Number of rows ", row_count)
print("Number of columns ", column_count)


# **=>Method to get null details,number of unique values etc**

# In[13]:


def nullDetails(data):
    stats = [];
    for i in data.columns:
        stats.append((i,data[i].nunique(),data[i].isnull().sum()/row_count,data[i].dtype))
    stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of null values','data_type'])
    l=stats_df.sort_values('Percentage of null values', ascending=False)
    return l[:30]


# In[14]:


nullDetails(data)


# **=> Uses of above data**
# - 1.if unique values is 1 then of no use drop
# - 2.if unique values is 2 and type is object then can labelencoded in true/false i.e 0/1
# - 3.if null values is >60% drop it.
# 

# ### =>Making copy of train data

# In[15]:


d = data.copy()
d.head()


# ### => Dropping the column where null value is > 60%
# 
# - We also tried to remove only those columns where null values > 90% but we don't see much change in end result.So we lower the thresold to 60%

# In[16]:


#Now drop the coloums
valid_cols = list(d.columns)
for i in d.columns:
    #print("current coloumn is",i);
    if(d[i].isnull().sum()/row_count > 0.6):
        print("current deleted coloumn is",i);
        valid_cols.remove(i)
        

    


# In[17]:


d = d[valid_cols]
valid_cols.remove('HasDetections')
test = test[valid_cols]


# In[18]:


d.info()


# ### So column with major null values removed,As you can see below

# In[19]:


nullDetails(d)


# **So now we have to manage the rest of null values.
# 
# - 1.where null is > 30%
#   * SmartScreen -> object
#   * OrganizationIdentifier -> float
# 

# In[20]:


print(d['SmartScreen'].value_counts())
print(d['SmartScreen'].isnull().sum())


# #### From the above output we could see that here are some different alternatives of same categories due to different spellings like off,OFF,off is same and similarly On,on is same.Replace them with the common ones to get the holistic value count.

# In[21]:


d = d.apply(lambda x: x.str.lower() if(x.dtype == 'object') else x)


# In[22]:


test = test.apply(lambda x: x.str.lower() if(x.dtype == 'object') else x)


# In[23]:


print(d['SmartScreen'].value_counts())
print(d['SmartScreen'].isnull().sum())


# **As we can see above,Now there is no case-sensitive values in any column**
# 
# 

# ### Now it's time for managening remaining null values.
# **For this we tried following methods:**
#   - **1.For Null values in Categorical column** 
#           * Replace null value with 0
#           *P.S we tried replacing null values with biggest unique value but got better result when replaced with 0.
#   - **2.For Null values in Numerical column**
#           * Replace null value with 0
#           *P.S we tried replacing null values with mean and median but got better result when replaced with 0.

# **=>Replacing null values in SmartScreen column**

# In[24]:


#d['SmartScreen']=d['SmartScreen'].fillna(d['SmartScreen'].value_counts().idxmax())
d['SmartScreen']=d['SmartScreen'].fillna('0')
test['SmartScreen']=test['SmartScreen'].fillna('0')


# **=>Replacing null values in OrganizationIdentifier column**

# In[25]:


print("Null values in OrganizationIdentifier",d['OrganizationIdentifier'].isnull().sum())


# In[26]:


# median  = d['OrganizationIdentifier'].mean()
# print(median)
d['OrganizationIdentifier'].fillna(0,inplace = True)
test['OrganizationIdentifier'].fillna(0,inplace = True)


# ### =>Now it's time to remove null values from rest of the Columns

# In[27]:


def replaceNullinInt(d,col):
    median = d[col].mean()
    d[col].fillna(median,inplace = True)
    


# In[28]:


def replaceNullinCat(d,col):
    #d[col].fillna(d[col].value_counts().idxmax(),inplace = True)
    d[col].fillna('0',inplace = True)
    


# In[29]:


for i in d.columns:
    if(d[i].dtype=='object'):
        replaceNullinCat(d,i)
    else:
        replaceNullinInt(d,i)

    


# In[30]:


for i in test.columns:
    if(test[i].dtype=='object'):
        replaceNullinCat(test,i)
    else:
        replaceNullinInt(test,i)


# ### Now there is no null values present in data.

# In[31]:


nullDetails(d)


# ### Studying and processing about Antivirus Products

# In[32]:


d['ProductName'].unique()


# In[33]:


d[d.HasDetections == 0].pivot_table('HasDetections',index='ProductName',columns='IsBeta',aggfunc='count').fillna(0)


# ## Inference :
# - 1.So most of Antivirus installed is Production Build.
# - 2.This shows that machine installed with non-beta version of "win8defender" experience most malwares failures.

# ### Let's explore the machines where "win8defender" is installed

# In[34]:


d.EngineVersion.unique()


# In[35]:


# Top 10 Popular EngineVersions of win8Defender
d[d.ProductName == 'win8defender'].EngineVersion.value_counts().nlargest(10)


# In[36]:


data.IsProtected = data.IsProtected.fillna(2.0)
# Here 2.0(float datatype) means NO AVP Installed


# In[37]:


protection=data[data.ProductName == 'win8defender'].groupby(['IsProtected','EngineVersion','HasDetections']).size().unstack()


# In[38]:


fig,ax = plt.subplots(3,1,figsize=(18,15))
for i in range(3):
    frame = protection.loc[i]
    frame = frame[frame.index.isin(frame.sum(1).nlargest(10).index)].sort_index()
    frame = frame.div(frame.sum(1),axis=0)
    frame.plot(kind='bar',ax=ax[i])
    
ax[0].set_title('Total top 10 Not Protected Machines by popular versions of Win8Defender')
ax[1].set_title('Total top 10 Protected Machines by popular versions of Win8Defender')
ax[2].set_title('Total top 10 Without any AVP Machines by popular versions of Win8Defender')
plt.subplots_adjust(hspace=1.)


# ### Total top 10 Not Protected Machines by popular versions of Win8Defender
# This horizontal bar graphs shows the percentage of total machines, categorized by the status of malware detections, running Win8Defender and an AV Program which is either currently active but not receiving updates or has no other AVP, sorted by the popular versions of Win8D. Here we can see that the machines running on version 1.1.151000.2, 1.1.14901.4, 1.1.14800.3 and 1.1.14700.5 had performed really well as the large % of the machines hadn't detected any malware/failure in their respective cases. However, the performance of machine with much updated ver. 1.1.15200.1 is not as great as their predecessors as there is a small gap between percent of machines that had either detected or not detected malwares.
# 
# ### Total top 10 Protected Machines by popular versions of Win8Defender
# This horizontal bar graphs shows the percentage of total machines, categorized by the status of malware detections, running Win8Defender and atleast one active and updated Anti Virus Program, sorted by the popular versions of Win8D. The graph depicts that as the version of Win8Defender gets updated after ver. 1.1.15000.2, the performance of machines degraded as the gap between the percentage of malware infected and not-infected machines decreases. One more interesting fact that one can infer by combining this graph's result with above graph's result is that there had been a significant decrease in the machine's performance running ver 1.1.15100.1 as more systems had detected malwares as compared to the previous scenario.
# 
# ### Total top 10 Without any AVP Machines by popular versions of Win8Defender
# This horizontal bar graphs shows the percentage of total machines, categorized by the status of malware detections, running Win8Defender and without any Anti Virus Program, sorted by the popular versions of Win8D. Here we can observe that machine solely running ver 1.1.14104.0 gets more prone to malwares as compared to the two previous cases.

# # Firewalls

# In[39]:


d.Firewall.value_counts()


# In[40]:


d[d.ProductName == 'win8defender'].groupby(['Firewall','HasDetections']).size().unstack()


# In[41]:


# Machines running OS ver Windows 8.1 or above & win8defender 
win8_1= d[(d.ProductName == 'win8defender') & (d.Firewall == 1)]
win8_1.shape


# In[42]:


d['Census_DeviceFamily'].unique()


# In[43]:


devicefamily = win8_1.groupby(['Census_DeviceFamily','HasDetections']).size().unstack()
devicefamily


# In[44]:


devicefamily = devicefamily.div(devicefamily.sum(1),axis=0)
devicefamily


# In[45]:


fig,ax = plt.subplots(figsize=(5,8))
devicefamily.plot(ax=ax,kind='barh',title='Machines Prone to malwares by Device Family')


# In[46]:


SmartSGrp = win8_1.groupby(['Census_DeviceFamily','SmartScreen','HasDetections']).size().unstack()
SmartSGrp


# In[47]:


fig,ax = plt.subplots(1,1,figsize=(12,6))
SmartSGrp.div(SmartSGrp.sum(1),axis=0).plot(kind='barh',ax=ax,colormap='plasma',title='Windows Desktop devices Malware Conditions by SmartScreen status')


# ### Now let's focus on our target 'HasDetections'

# In[48]:


#number of yes and no in value counts
d['HasDetections'].value_counts()


# In[49]:


d['HasDetections'].value_counts().plot(kind = 'pie', figsize=(9,6), colormap="coolwarm")


# **we have two problems here:-**
# - 1.hasdetection supposed to be boolean i.e only 0 and 1,but here there are diff values too.
# - 2.data is not balanced i.e value_count of 0 is >> value_count of 1.

# so for now deleting the rows where values are diff then 0 and 1

# In[50]:


d = d[d.HasDetections <2]


# In[51]:


a = [0,1]
d = d[d.HasDetections.isin(a)]


# In[52]:


d['HasDetections'].value_counts()


# In[53]:


d['HasDetections'].value_counts().plot(kind = 'pie', figsize=(9,6), colormap="coolwarm")


# **Now only two values but highly unbalanced.
# So following methods we are going to try**
#  - 1.data undersampling
#  - 2.tree based training model

# ## Encoding
# 

# ### Since there is lot of categorical value we have to do encoding.
# **We tried LabelEncoding and frequencyEncoding.But didn't get expected result wihh frequency encoding,so proceded with only labelEncoding.**

# **First dropping the 'MachineIdentifier' column,Since it is not a feature**

# In[54]:


d = d.drop(['MachineIdentifier'], axis = 1) 
test = test.drop(['MachineIdentifier'], axis = 1) 


# In[55]:


cat_cols = [];
for col in d.columns:
    if col not in ['MachineIdentifier','HasDetections'] and str(d[col].dtype) == 'object':
        cat_cols.append(col)
len(cat_cols)


# ### =>Frequency encoding
# For variables with large cardinality, an efficient encoding consists in ranking the categories with respect to their frequencies. These variables are then treated as numerical.

# In[56]:


# uni_col = [] #for highly unique values -> frequency encoding
# for col in cat_cols:
#     if d[col].nunique() >500:
#         print(col, d[col].nunique())
#         uni_col.append(col)
#         cat_cols.remove(col)


# In[57]:


# valid_cols = list(d.columns)
# for i in uni_col:
#     #print("current col is",i)
#     freq_d = (d.groupby(i).size()) / len(d)
#     freq_t = (test.groupby(i).size()) / len(test)
#     #print("freq is ",freq)
#     d[i] = d[i].map(lambda x: freq_d.get(x,np.nan))
#     test[i] = test[i].map(lambda x: freq_t.get(x,np.nan))
#     #same do with test
#     #drop cat column
#     #valid_cols.remove(i);


# In[58]:


# data  = data[valid_cols]

# cat_cols.remove("MachineIdentifier")
# test_cols = valid_cols[:]
# test_cols.remove("HasDetections")#since it is id,not any parameter.
# test = test[test_cols]


# ### Label Encoding

# In[59]:


lencoder = LabelEncoder() 
valid_cols = list(data.columns)
for i in cat_cols:
    print("current col is",i)
    d[i] = lencoder.fit_transform(d[i].astype('str'))
    test[i] = lencoder.fit_transform(test[i].astype('str'))
        #valid_cols.remove(i)


# In[60]:


d.info()


# **As you can see above,there is no categorical values left.**

# ### Label Encoding done on above columns

# **Dropping any remaining Null values**

# In[61]:


d = d.dropna(how = 'any')


# ### Dividing test data in X and Y

# In[62]:



X = d.drop(["HasDetections"], axis = 1) 
Y = d["HasDetections"]


# In[63]:


Y.head()


# In[64]:


X.info()


# ### Undersampling

# In[65]:


Y.value_counts()


# **So currently data is highly unbalanced towards 0**

# **We tried various undersampling methos like:**
# - RandomUnderSampler
# - NeighbourhoodCleaningRule
# - ClusterCentroids
# - TomekLinks
# - NearMiss
# 
# Ref - [Resampling to properly handle imbalanced datasets](https://heartbeat.fritz.ai/resampling-to-properly-handle-imbalanced-datasets-in-machine-learning-64d82c16ceaa)

# In[66]:


# from imblearn.under_sampling import NeighbourhoodCleaningRule

# # create the object to resample the majority class.
# rus = NeighbourhoodCleaningRule(sampling_strategy="majority")


# In[67]:


# from imblearn.under_sampling import ClusterCentroids

# # instantiate the object with the right ratio.
# res = ClusterCentroids(sampling_strategy="auto")


# In[68]:


# # import the TomekLinks object.
# from imblearn.under_sampling import TomekLinks

# # instantiate the object with the right ratio strategy.
# rus = TomekLinks(sampling_strategy='majority')


# In[69]:


# from imblearn.under_sampling import NearMiss

# # create the object with auto
# rus = NearMiss(sampling_strategy="not minority")

# #it does kind of overfitting so ignoring it.


# In[70]:


from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(sampling_strategy='majority')


# In[71]:


X, Y = rus.fit_resample(X, Y)


# In[72]:


Y.value_counts()


# **Now our train data got balanced**

# ## Model Training

# In[73]:


import gc
from sklearn.model_selection import  GridSearchCV
import lightgbm as lgb
from tqdm import tqdm
from sklearn.metrics import (roc_curve, auc, accuracy_score)


# ### Spliting data in test and train

# In[74]:


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)


# ### For training we tried different models like LGBM,LR,XGBoost etc but finally we got result using XGBoost

# ### LGBM

# In[100]:


# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(X_train)
x_test = sc.transform(X_test)


# In[101]:



def lgbm(x_train,y_train,param):
    d_train = lgb.Dataset(x_train, label=y_train)
    clf = lgb.train(param, d_train, 100)
    return clf
    


# In[102]:


params = {'num_leaves': 80,
         'min_data_in_leaf': 20, 
         'objective':'binary',
         'boosting_type': "gbdt", 
         'max_depth': 10,
         'learning_rate': 0.002,
         "boosting": "gbdt",
         "feature_fraction": 0.8,
         "bagging_freq": 1,
         "bagging_fraction": 0.8 ,
         "bagging_seed": 11,
         "metric": 'auc',
         "lambda_l1": 0.1,
         "random_state": 42,
         "verbosity": -1
            }


# In[104]:


clf_lgbm = lgbm(x_train,y_train,params)


# In[105]:


#Prediction
y_pred_lgbm=clf_lgbm.predict(x_test)
#convert into binary values
for i in range(len(y_pred_lgbm)):
    if y_pred_lgbm[i]>=.5:       # setting threshold to .5
        y_pred_lgbm[i]=1
    else:  
        y_pred_lgbm[i]=0


# In[132]:



print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred_lgbm)))
print('Precision Score : ' + str(precision_score(y_test,y_pred_lgbm)))
print('Recall Score : ' + str(recall_score(y_test,y_pred_lgbm)))
print('F1 Score : ' + str(f1_score(y_test,y_pred_lgbm)))

from sklearn.metrics import confusion_matrix
print('Confusion Matrix : \n' + str(confusion_matrix(y_test,y_pred_lgbm)))
from sklearn.metrics import roc_auc_score
print('ROC Score : ' + str(roc_auc_score(y_test,y_pred_lgbm)))


# ### LR

# In[107]:


from sklearn.linear_model import LogisticRegression
def LR(X_train,y_train):
    logisticRegr = LogisticRegression()
    logisticRegr.fit(X_train,y_train)
    return logisticRegr
    


# In[108]:


clf_lr = LR(X_train,y_train)


# In[110]:


y_pred_lr = clf_lr.predict(X_test)


# In[131]:


print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred_lr)))
print('Precision Score : ' + str(precision_score(y_test,y_pred_lr)))
print('Recall Score : ' + str(recall_score(y_test,y_pred_lr)))
print('F1 Score : ' + str(f1_score(y_test,y_pred_lr)))

from sklearn.metrics import confusion_matrix
print('Confusion Matrix : \n' + str(confusion_matrix(y_test,y_pred_lr)))
from sklearn.metrics import roc_auc_score
print('ROC Score : ' + str(roc_auc_score(y_test,y_pred_lr)))


# ### XGBoost

# In[87]:


import xgboost as xgb
from sklearn.model_selection import StratifiedKFold

model = xgb.XGBClassifier()


# ### Grid Search for Params Optimization

# In[88]:


learning_rate = [0.0001,0.001,0.01,0.03,0.05, 0.2, 0.3]
param_grid = dict(learning_rate=learning_rate)
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=3)
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X_train,y_train)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']

means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))


# In[89]:


param_grid={'max_depth': np.arange(8,12,1) }
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=3)
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X_train,y_train)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))


# In[90]:


param_grid={'min_child_weight': np.arange(6,11,1) }
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=3)
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X_train,y_train)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))


# In[91]:


param_grid={'colsample_bytree': np.arange(0.3,1.1,0.1) }
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=3)
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X_train,y_train)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))


# In[92]:


param_grid={'subsample': np.arange(0.3,1.1,0.1) }
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=3)
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X_train,y_train)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))


# In[93]:


param_grid={'scale_pos_weight': np.arange(0,5,1) }
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=3)
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X_train,y_train)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_)) 


# In[94]:


param_grid={'reg_alpha': np.arange(0.1,1.0,0.1) }
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=3)
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X_train,y_train)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_)


# In[ ]:


param_grid={'reg_lambda': np.arange(1,5,1) }
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=3)
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X_train,y_train)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))


# In[ ]:


param_grid={'gamma': np.arange(0.1,0.5,0.1) }
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=3)
grid_search = GridSearchCV(model, param_grid, scoring="neg_log_loss", n_jobs=-1, cv=kfold)
grid_result = grid_search.fit(X_train,y_train)
# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))


# In[95]:


clf_xgb = xgb.XGBClassifier(learning_rate=0.1, 
                            n_estimators=20000, 
                            max_depth=8,
                            min_child_weight=10,
                            gamma=0.3,
                            subsample=1,
                            colsample_bytree=0.3,
                            nthread=-1,
                            scale_pos_weight=1,
                            reg_alpha = 0.6,
                            reg_lambda = 3,
                            seed=42)

clf_xgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], 
            early_stopping_rounds=100, eval_metric='auc', verbose=100)

y_pred = clf_xgb.predict(X_test)


# ### Accuracy on X_train

# In[96]:


y_pred_train = clf_xgb.predict(X_train)


# In[129]:


from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score
print('Accuracy Score : ' + str(accuracy_score(y_train,y_pred_train)))
print('Precision Score : ' + str(precision_score(y_train,y_pred_train)))
print('Recall Score : ' + str(recall_score(y_train,y_pred_train)))
print('F1 Score : ' + str(f1_score(y_train,y_pred_train)))

from sklearn.metrics import confusion_matrix
print('Confusion Matrix : \n' + str(confusion_matrix(y_train,y_pred_train)))
from sklearn.metrics import roc_auc_score
print('ROC Score : ' + str(roc_auc_score(y_train,y_pred_train)))


# ### Accuracy on X_test

# In[98]:


#Distribution of y test
print('y actual : \n' +  str(y_test.value_counts()))

#Distribution of y predicted
print('y predicted : \n' + str(pd.Series(y_pred).value_counts()))


# In[133]:



print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred)))
print('Precision Score : ' + str(precision_score(y_test,y_pred)))
print('Recall Score : ' + str(recall_score(y_test,y_pred)))
print('F1 Score : ' + str(f1_score(y_test,y_pred)))

#Dummy Classifier Confusion matrix
from sklearn.metrics import confusion_matrix
print('Confusion Matrix : \n' + str(confusion_matrix(y_test,y_pred)))
from sklearn.metrics import roc_auc_score
print('ROC Score : ' + str(roc_auc_score(y_test,y_pred)))


# ### Test implementation

# In[ ]:


#Prediction


# In[115]:


y_pred_test = clf_xgb.predict(test)


# In[116]:


print(np.unique(y_pred_test))
unique, frequency = np.unique(y_pred_test, return_counts = True) 
print(frequency)


# In[117]:


sample= pd.read_csv("../input/malware-detection-dk/sample_submission.csv")
sample.head()


# In[118]:


test1= pd.read_csv("../input/malware-detection-dk/test.csv")


# In[119]:


my_submission = pd.DataFrame({'MachineIdentifier': test1.MachineIdentifier, 'HasDetections': y_pred_test})
my_submission.to_csv('submission_10.csv', index=False)


# In[120]:


sub= pd.read_csv("./submission_10.csv")


# In[121]:


sub.head()


# In[122]:


sub['HasDetections'].value_counts()

